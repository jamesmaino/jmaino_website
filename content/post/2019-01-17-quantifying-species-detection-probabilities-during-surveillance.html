---
title: Tutorial on quantifying species detection probabilities during surveillance with stan in R
author: ~
date: '2019-01-17'
slug: tutorial-on-quantifying-species-detection-probabilities-during-surveillance
categories: []
tags: ["bayesian", "stan", "monitorring", "resistance", "biosecurity", "exponential", "logistic", "R"]
header:
  caption: 'Quantifying detection of eggs'
  image: 'eastereggs.jpg'
---



<p>A practical question in species surveillance is “How much search effort is required for detection?”. This can be quantified under controlled conditions where the number and location of target species are known and participants are recruited to see how success rate varies.</p>
<p>Let’s use an example of an easter egg hunt where the adult (the researcher) wants to quantify how much effort it takes a child (the participant) to find an easter egg. Imagine that we let each child complete the easter egg course, and it is reset to the same condition before each new child.</p>
<pre class="r"><code>library(tidyverse)
library(rethinking)</code></pre>
<p>Here is a data summary of a such a trial <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<table>
<thead>
<tr class="header">
<th align="right">participant</th>
<th align="left">experience</th>
<th align="right">search_time_h</th>
<th align="right">area_searched_ha</th>
<th align="right">search_effort_h_ha</th>
<th align="right">no_sets_encountered</th>
<th align="right">no_sets_detected</th>
<th align="right">percent_detected</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="left">yes</td>
<td align="right">1.67</td>
<td align="right">0.29</td>
<td align="right">5.69</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">100</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="left">yes</td>
<td align="right">2.17</td>
<td align="right">0.25</td>
<td align="right">8.84</td>
<td align="right">6</td>
<td align="right">5</td>
<td align="right">83</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="left">yes</td>
<td align="right">1.42</td>
<td align="right">0.45</td>
<td align="right">3.14</td>
<td align="right">8</td>
<td align="right">5</td>
<td align="right">63</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="left">no</td>
<td align="right">1.08</td>
<td align="right">0.42</td>
<td align="right">2.57</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">60</td>
</tr>
<tr class="odd">
<td align="right">7</td>
<td align="left">no</td>
<td align="right">1.00</td>
<td align="right">0.90</td>
<td align="right">1.11</td>
<td align="right">13</td>
<td align="right">7</td>
<td align="right">54</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="left">yes</td>
<td align="right">1.42</td>
<td align="right">0.84</td>
<td align="right">1.69</td>
<td align="right">15</td>
<td align="right">5</td>
<td align="right">33</td>
</tr>
<tr class="odd">
<td align="right">11</td>
<td align="left">yes</td>
<td align="right">1.07</td>
<td align="right">0.65</td>
<td align="right">1.64</td>
<td align="right">9</td>
<td align="right">3</td>
<td align="right">33</td>
</tr>
<tr class="even">
<td align="right">12</td>
<td align="left">no</td>
<td align="right">1.35</td>
<td align="right">0.20</td>
<td align="right">6.74</td>
<td align="right">3</td>
<td align="right">1</td>
<td align="right">33</td>
</tr>
<tr class="odd">
<td align="right">8</td>
<td align="left">yes</td>
<td align="right">0.92</td>
<td align="right">1.20</td>
<td align="right">0.77</td>
<td align="right">23</td>
<td align="right">7</td>
<td align="right">30</td>
</tr>
<tr class="even">
<td align="right">10</td>
<td align="left">no</td>
<td align="right">1.00</td>
<td align="right">0.76</td>
<td align="right">1.32</td>
<td align="right">15</td>
<td align="right">4</td>
<td align="right">27</td>
</tr>
<tr class="odd">
<td align="right">9</td>
<td align="left">no</td>
<td align="right">1.28</td>
<td align="right">0.24</td>
<td align="right">5.43</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">20</td>
</tr>
<tr class="even">
<td align="right">5</td>
<td align="left">yes</td>
<td align="right">1.17</td>
<td align="right">1.43</td>
<td align="right">0.81</td>
<td align="right">22</td>
<td align="right">2</td>
<td align="right">9</td>
</tr>
</tbody>
</table>
<p>This table shows some child level characteristics that might effect detection, such as whether or not they have had any prior experience looking for easter eggs. But suppose we want to control for some property of the hidden easter egg, such as the size of the egg in grams, where we might pressume that larger eggs are easier to find. Here I separate out the data into Bernoulli form (1s for detections and 0s for misses) and assign a random egg size between 10 and 100 g.</p>
<pre class="r"><code>set.seed(111) # fix random numbers
# convert summary data to long format (Bernoulli trials)
total_eggs = 23
db = d %&gt;%
  rowwise() %&gt;%
  mutate(detection = list(rep(c(0, 1), c(no_sets_encountered - no_sets_detected, no_sets_detected)))) %&gt;%
  dplyr::select(-no_sets_encountered,-no_sets_detected, -percent_detected) %&gt;%
  unnest() %&gt;%
  
  # assign id to eggs and then random size
  group_by(participant) %&gt;%
  mutate(egg_id = sample(1:total_eggs,n(),replace = FALSE)) %&gt;%
  ungroup %&gt;%
  mutate(egg_size = runif(total_eggs, 10,100)[egg_id]) %&gt;%
  mutate(experience = ifelse(experience == &#39;yes&#39;, 1, 0)) %&gt;%
  mutate(log_search_effort_h_ha = log(search_effort_h_ha)) %&gt;%
  
  # it is good practice to use centred variable (mean = 0) for parameter interpretation and fitting
  mutate(egg_size_c = egg_size - mean(egg_size)) %&gt;%
  as.data.frame
  
knitr::kable(head(db))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">participant</th>
<th align="right">experience</th>
<th align="right">search_time_h</th>
<th align="right">area_searched_ha</th>
<th align="right">search_effort_h_ha</th>
<th align="right">detection</th>
<th align="right">egg_id</th>
<th align="right">egg_size</th>
<th align="right">log_search_effort_h_ha</th>
<th align="right">egg_size_c</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1.67</td>
<td align="right">0.29</td>
<td align="right">5.69</td>
<td align="right">1</td>
<td align="right">14</td>
<td align="right">88.35757</td>
<td align="right">1.738710</td>
<td align="right">27.820235</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1.67</td>
<td align="right">0.29</td>
<td align="right">5.69</td>
<td align="right">1</td>
<td align="right">2</td>
<td align="right">24.79464</td>
<td align="right">1.738710</td>
<td align="right">-35.742694</td>
</tr>
<tr class="odd">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1.67</td>
<td align="right">0.29</td>
<td align="right">5.69</td>
<td align="right">1</td>
<td align="right">11</td>
<td align="right">92.79748</td>
<td align="right">1.738710</td>
<td align="right">32.260140</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1.67</td>
<td align="right">0.29</td>
<td align="right">5.69</td>
<td align="right">1</td>
<td align="right">10</td>
<td align="right">56.31577</td>
<td align="right">1.738710</td>
<td align="right">-4.221572</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">2.17</td>
<td align="right">0.25</td>
<td align="right">8.84</td>
<td align="right">0</td>
<td align="right">9</td>
<td align="right">65.42770</td>
<td align="right">2.179287</td>
<td align="right">4.890362</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">2.17</td>
<td align="right">0.25</td>
<td align="right">8.84</td>
<td align="right">1</td>
<td align="right">22</td>
<td align="right">17.12628</td>
<td align="right">2.179287</td>
<td align="right">-43.411054</td>
</tr>
</tbody>
</table>
<p>So now that we have some characterics of the child (e.g. search effort), and each hidden egg (e.g. size) that are expected to affect detection probabilities it is time to model it.</p>
<p>Each encounter (<span class="math inline">\(X_{i,j}\)</span>) with an egg j by child i can be considered a draw from a Bernoulli distribution with parameter <span class="math inline">\(p_{i,j}\)</span>, which is the probability of detecting the egg given it was present:</p>
<p><span class="math display">\[X_{i,j} \sim \textrm{dbern}(p_{i,j})\]</span></p>
<p>The are many ways to model the probability of detection. One is to assume an exponential relationship with search effort where <span class="math inline">\(p_{i,j}\)</span> is given by:</p>
<p><span class="math display">\[p_{i,j} = 1 - exp(-\lambda_{i,j} f_j)\]</span></p>
<p>where <span class="math inline">\(f_i\)</span> is the search effort per unit area of child i, and <span class="math inline">\(\lambda_{i,j}\)</span>i,j is the rate of detection of plant set j by observer i, modelled as a log-linear function of factors identified as likely to influence detectability or:</p>
<p><span class="math display">\[\ln(\lambda_{i,j}) = a + f(s_j) + g(e_i) + \textrm{obs}_i\]</span>
where <span class="math inline">\(a\)</span> is the intercept term, <span class="math inline">\(f(s_j)\)</span> is the effect of egg size, and <span class="math inline">\(g(e_i)\)</span> is the effect child experience. The reference level of child experience is ‘no experience’.</p>
<p>For an intuitive explanation of commonly used probability distributions, see <a href="/post/generating-probability-distribution-with-natural-examples">this previous post</a> and <a href="/post/bayesian-and-frequentist-logistic-regression-in-r">this one on bayesian vs. frequentist binomial responses</a>.</p>
<p>On to fitting. First, get some reasonable initial values for the model.</p>
<pre class="r"><code>a = 0.17  
b = 0.001
c = 0.125
db %&gt;% 
  mutate(pred_hauser =  1 - exp(-(a + b*egg_size_c + c*experience) * search_effort_h_ha)) %&gt;% 
  dplyr::select(pred_hauser, detection) %&gt;%
  head</code></pre>
<pre><code>##   pred_hauser detection
## 1   0.8406811         1
## 2   0.7712611         1
## 3   0.8446555         1
## 4   0.8088180         1
## 5   0.9294216         0
## 6   0.8918295         1</code></pre>
<p>Then fit the model using Richard McElreath’s <code>rethinking</code> package and <code>rstan</code>.</p>
<pre class="r"><code>m1 &lt;- rethinking::map2stan(
  alist(
    detection ~ dbinom( 1 , 1 - exp(-lambda_f) ),
    lambda_f &lt;-  (a + b*egg_size_c + c*experience + obs[participant]) * search_effort_h_ha ,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    c ~ dnorm(0, 10),
    obs[participant] ~ dnorm(0, sigma) ,
    sigma ~ dcauchy(0,1)
  ),
  data = db, 
  start = list(a=0.17, b = 0.001, c=0.125, sigma = 0.3) ,
  # constraints=list(d =&quot;lower=0&quot;),
  chains=1 , iter=10000 , warmup=1000
)
save(m1, file = &#39;data/detection_prob/m1.Rdata&#39;)</code></pre>
<pre class="r"><code># summary output
precis(m1)</code></pre>
<pre><code>##       Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## a     0.26   0.08       0.12       0.38   157 1.01
## b     0.00   0.00       0.00       0.00  2059 1.00
## c     0.06   0.11      -0.11       0.22   188 1.00
## sigma 0.13   0.06       0.02       0.21   160 1.00</code></pre>
<pre class="r"><code># plot detection probability vs. effort
pars = precis(m1)@output$Mean
names(pars) = rownames(precis(m1)@output)
obs = precis(m1, depth = 2)@output$Mean[5:16]
pred_line =  expand.grid(search_effort_h_ha = seq(0, 10, length = 100),
                         egg_size = c(60.5),
                         experience = 0:1, 
                         participant = 1) %&gt;% # placeholder participant
  mutate(egg_size_c = egg_size - mean(db$egg_size))
# create matrix of zeros getting mean of random effect
participant_zeros = matrix(0, 1000, max(d$participant)) 
mu &lt;- link(m1, data=pred_line, replace=list(obs=participant_zeros))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>pred_line$pred &lt;- 1 - exp(-apply( mu , 2 , mean ))
pred_line[,c(&#39;lower&#39;,&#39;upper&#39;)] &lt;- t(1 - exp(-apply(mu, 2, PI, prob = 0.89)))

# predictions for each participant with random effect (at mean egg size) 
d = d %&gt;%
  mutate(pred = 1 - exp(-((pars[&#39;a&#39;] + pars[&#39;b&#39;]*0 + pars[&#39;c&#39;]*(experience==&#39;yes&#39;) + obs[participant]) * search_effort_h_ha)))
ggplot(pred_line, aes(search_effort_h_ha, pred, colour = factor(experience))) +
  geom_line(aes(linetype = factor(experience))) +
  geom_line(aes(y=lower, linetype = factor(experience))) +
  geom_line(aes(y=upper, linetype = factor(experience))) +
  geom_ribbon(aes(ymin=lower,ymax=upper,fill = factor(experience), color = NULL), alpha = 0.2) +
  geom_point(data = d, aes(linetype = NULL), colour = &#39;white&#39;, shape = 1) +
  geom_point(data = d, aes(y = percent_detected/100), colour = &#39;white&#39;) +
  ylab(&#39;predicted detection probability&#39;) +
  xlab(&#39;search effort, h/ha&#39;) +
  mydarktheme</code></pre>
<p><img src="/post/2019-01-17-quantifying-species-detection-probabilities-during-surveillance_files/figure-html/unnamed-chunk-7-1.png" width="672" />
<strong>Figure 1.</strong> Plot of model 1 predicted detection probability as a function of search effort. Filled circles show observed detected percentages by each participant, while hollow circles show predicted probabilities for each observer’s measured effort level and fitted random effect (egg size is set to mean). Shaded regions shows 89% credible interval.</p>
<p>In choosing to implement this model in stan, I found myself a world of hurt. I have never seen so many error messages as I fumbled my way through red words trying to make a log-link function work with a binomial regression (or some other way to restrict the value of <span class="math inline">\(\lambda\)</span> to positive values else break the contraint <span class="math inline">\(0&lt;p&lt;1\)</span>). Even the output above with Rhat &gt; 1 hints that there may be some issues. Here is a helpful thread <a href="https://discourse.mc-stan.org/t/binomial-regression-with-a-log-link/4102/8">a thread in a stan forum</a>. The key piece of insight is that probabilities have to be between 0 and 1. If you have some linear function <code>y = a + x * b</code>, which can take on any real value, then <code>inv_logit(y)</code> is always a valid probability, but not <code>exp(y)</code>.</p>
<p>For this reason, I explored a more conventional approach using the <code>logit</code> link function where our linear function is framed around the log odds ratio <span class="math inline">\(\ln(\frac{p}{1-p})\)</span>. I ended up with a pretty similar model.</p>
<p><span class="math display">\[p_{i,j} = \frac{1}{1 - exp(-y_{i,j})}\]</span>
where</p>
<p><span class="math display">\[y_{i,j} = a + f(s_j) + g(e_i) + d \log(f_i) + \textrm{obs}_i\]</span>
and the variables are defined as before, with the exception that <span class="math inline">\(h(f_i) = d \log(f_i)\)</span>.</p>
<p>Expressing the equation in terms of the odds ratio helps interpretabilty.</p>
<p><span class="math display">\[\frac{p}{1-p} = exp(a + f(s_j) + g(e_i) + \textrm{obs}_i)f_i^d\]</span></p>
<p>The odds of detection (rather than probability) scales with survey effort to the power of d.</p>
<p>Reasonable initial values are less important in this model as the logit function properly restricts fitted probabilities to between 0 and 1.</p>
<pre class="r"><code>m2 &lt;- rethinking::map2stan(
  alist(
    detection ~ dbinom( 1 , p ),
    logit(p) &lt;-  a + b*egg_size_c + c*experience + d*log_search_effort_h_ha + obs[participant]  ,
    a ~ dnorm(0, 10),
    b ~ dnorm(0, 10),
    c ~ dnorm(0, 10),
    d ~ dnorm(0, 10),
    obs[participant] ~ dnorm(0, sigma) ,
    sigma ~ dcauchy(0,1)
  ),
  data = db, 
  chains=1 , iter=10000 , warmup=1000
)
save(m2, file = &#39;data/detection_prob/m2.Rdata&#39;)</code></pre>
<pre class="r"><code># summarise model 
precis(m2)</code></pre>
<pre><code>##        Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
## a     -1.23   0.66      -2.21      -0.21  2724    1
## b      0.01   0.01       0.00       0.02  9688    1
## c      0.33   0.69      -0.80       1.35  2727    1
## d      0.95   0.44       0.27       1.65  4223    1
## sigma  0.74   0.42       0.10       1.25  1057    1</code></pre>
<pre class="r"><code># plot detection probability vs effort
pars = precis(m2)@output$Mean
names(pars) = rownames(precis(m2)@output)
obs = precis(m2, depth = 2)@output$Mean[4:16]
pred_line =  expand.grid(search_effort_h_ha = seq(0, 10, length = 100),
                         egg_size = c(60.5),
                         experience = 0:1,
                         participant = 1) %&gt;% # participant place holder
  mutate(egg_size_c = egg_size - mean(db$egg_size)) %&gt;%
  mutate(log_search_effort_h_ha = log(search_effort_h_ha))

# create matrix of zeros getting mean of random effect
participant_zeros = matrix(0, 1000, max(d$participant)) 
mu &lt;- link(m2, data=pred_line, replace=list(obs=participant_zeros))</code></pre>
<pre><code>## [ 100 / 1000 ]
[ 200 / 1000 ]
[ 300 / 1000 ]
[ 400 / 1000 ]
[ 500 / 1000 ]
[ 600 / 1000 ]
[ 700 / 1000 ]
[ 800 / 1000 ]
[ 900 / 1000 ]
[ 1000 / 1000 ]</code></pre>
<pre class="r"><code>pred_line$pred &lt;- (apply( mu , 2 , mean ))
pred_line[,c(&#39;lower&#39;,&#39;upper&#39;)] &lt;- t((apply( mu , 2 , PI, prob = 0.89 )))

# predictions for each participant with random effect (at mean egg size) 
d = d %&gt;%
  mutate(pred = (inv_logit(pars[&#39;a&#39;] + pars[&#39;b&#39;]*0 + pars[&#39;c&#39;]*(experience==&#39;yes&#39;) + pars[&#39;d&#39;]*log(search_effort_h_ha) + obs[participant])))
ggplot(pred_line, aes(search_effort_h_ha, pred, colour = factor(experience))) +
  geom_line(aes(linetype = factor(experience))) +
  geom_line(aes(y=lower, linetype = factor(experience))) +
  geom_line(aes(y=upper, linetype = factor(experience))) +
  geom_ribbon(aes(ymin=lower,ymax=upper,fill = factor(experience), color = NULL), alpha = 0.2) +
  geom_point(data = d, aes(linetype = NULL), colour = &#39;white&#39;, shape = 1) +
  geom_point(data = d, aes(y = percent_detected/100), colour = &#39;white&#39;) +
  ylab(&#39;predicted detection probability&#39;) +
  xlab(&#39;search effort, h/ha&#39;) +
  mydarktheme</code></pre>
<p><img src="/post/2019-01-17-quantifying-species-detection-probabilities-during-surveillance_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><strong>Figure 2.</strong> Plot of model 2 predicted detection probability as a function of search effort. Filled circles show observed detected percentages by each participant, while hollow circles show predicted probabilities for each observer’s measured effort level and fitted random effect (egg size is set to mean). Shaded regions shows 89% credible interval.</p>
<p>Look at that sweet convergence.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This is real data but instead of easter eggs, participants were asked to survey for hawkweed in the Victorian alps. Data is from Moore, J. L., Hauser, C. E., Bear, J. L., Williams, N. S. and McCarthy, M. A. (2011), Estimating detection–effort curves for plants using search experiments. Ecological Applications, 21: 601-607. <a href="doi:10.1890/10-0590.1" class="uri">doi:10.1890/10-0590.1</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
